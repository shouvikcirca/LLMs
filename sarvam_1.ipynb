{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shouvikcirca/LLMs/blob/audio/sarvam_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install moviepy\n",
        "! pip install librosa\n",
        "! ! pip install python-dotenv\n",
        "! pip install openai"
      ],
      "metadata": {
        "id": "F4WO1pXJjDrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "id": "NrQKEWbijDuG",
        "outputId": "9259ab86-ca04-4b7b-e56c-a6370893848c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls drive/MyDrive/LLM"
      ],
      "metadata": {
        "id": "lG4vanw1jDw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import glob"
      ],
      "metadata": {
        "id": "7WxFTIV7kI1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_files = glob.glob('drive/MyDrive/LLM/*mp3')"
      ],
      "metadata": {
        "id": "HImc-_rgjDzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y, sr = librosa.load(audio_files[0])"
      ],
      "metadata": {
        "id": "t1OTuzOGjD2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling rate of loaded audio -> 22050"
      ],
      "metadata": {
        "id": "Ez_QS3EqjD46",
        "outputId": "48af920a-efdb-40a9-fadb-ac732ba1c9a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22050"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "_= load_dotenv('drive/MyDrive/env')\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ],
      "metadata": {
        "id": "_bBXoMWQjD7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(api_key = openai.api_key)\n",
        "\n",
        "audio_file = open(audio_files[0], \"rb\")\n",
        "transcript = client.audio.transcriptions.create(\n",
        "  file=audio_file,\n",
        "  model=\"whisper-1\",\n",
        "  response_format=\"verbose_json\",\n",
        "  timestamp_granularities=[\"word\"]\n",
        ")"
      ],
      "metadata": {
        "id": "jmgEO76tjD-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in transcript.words[:5]:\n",
        "  print(w['word'],w['start'],w['end'])"
      ],
      "metadata": {
        "id": "qMdjKhsAl57l",
        "outputId": "21455c29-f285-4512-f64c-ce57153f796f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Congratulations 0.14000000059604645 0.6600000262260437\n",
            "to 0.6600000262260437 0.9800000190734863\n",
            "you 0.9800000190734863 1.1200000047683716\n",
            "Mr 1.1200000047683716 1.3600000143051147\n",
            "Raghavan 1.4199999570846558 1.6200000047683716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getTimeUnits(s):\n",
        "  return (\n",
        "      int(s/3600),\n",
        "      int(s/60),\n",
        "      int(s),\n",
        "      int((s - int(s))*1000)\n",
        "  )"
      ],
      "metadata": {
        "id": "vXfvRYinqUsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def displaySubs(st):\n",
        "  for c in st:\n",
        "    if c=='\\n':\n",
        "      print()\n",
        "    else:\n",
        "      print(c,end='')"
      ],
      "metadata": {
        "id": "eseSZ7hJtCSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subtitleText = ''\n",
        "for wcount in range(len(transcript.words[:10])):\n",
        "  sh,sm,ss,sms = getTimeUnits(transcript.words[wcount]['start'])\n",
        "  eh,em,es,ems = getTimeUnits(transcript.words[wcount]['end'])\n",
        "  # print(str(wcount+1))\n",
        "  subtitleText+=(str(wcount+1)+'\\n')\n",
        "  # print(sh,':',sm,':',ss,',',sms,'-->',eh,':',em,':',es,',',ems)\n",
        "  subtitleText+='{}:{}:{},{} --> {}:{}:{},{}\\n'.format(sh,sm,ss,sms,eh,em,es,ems)\n",
        "  # print(transcript.words[wcount]['word'],'\\n')\n",
        "  subtitleText+=(transcript.words[wcount]['word']+'\\n\\n')"
      ],
      "metadata": {
        "id": "DNheetkcoh2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displaySubs(subtitleText)"
      ],
      "metadata": {
        "id": "muQcZkk_sUai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forming Chunks"
      ],
      "metadata": {
        "id": "Uj4u-b9awCXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "currentChunk = []\n",
        "wholeChunk = []\n",
        "chunkCount = 1\n",
        "chunkInterval = 15\n",
        "\n",
        "for w in transcript.words:\n",
        "  if int(w['start'])>(chunkCount*chunkInterval):\n",
        "    wholeChunk.append(currentChunk)\n",
        "    currentChunk = [w]\n",
        "    chunkCount+=1\n",
        "  elif int(w['end'])>(chunkCount*chunkInterval):\n",
        "    wholeChunk.append(currentChunk)\n",
        "    currentChunk = [w]\n",
        "    chunkCount+=1\n",
        "  else:\n",
        "    currentChunk.append(w)\n",
        "\n",
        "wholeChunk.append(currentChunk)"
      ],
      "metadata": {
        "id": "iWrzxIa9wB6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making sure each word has been mapped into one chunk\n",
        "s = 0\n",
        "for chunk in wholeChunk:\n",
        "  s+=len(chunk)\n",
        "\n",
        "s == len(transcript.words)"
      ],
      "metadata": {
        "id": "ETelGnwdyHbM",
        "outputId": "4b268e1f-5d89-49df-e90f-1290c8cedada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subtitleTextChunk = ''\n",
        "for c in range(len(wholeChunk)):\n",
        "  startTime = wholeChunk[c][0]['start']\n",
        "  endTime = wholeChunk[c][-1]['end']\n",
        "  sh,sm,ss,sms = getTimeUnits(startTime)\n",
        "  eh,em,es,ems = getTimeUnits(endTime)\n",
        "  subtitleTextChunk+=(str(c+1)+'\\n')\n",
        "  subtitleTextChunk+='{}:{}:{},{} --> {}:{}:{},{}\\n'.format(sh,sm,ss,sms,eh,em,es,ems)\n",
        "  transcription = ' '.join(map(lambda d: d['word'], wholeChunk[c]))\n",
        "  subtitleTextChunk+=(transcription+'\\n\\n')\n"
      ],
      "metadata": {
        "id": "_6ty_hYbypjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displaySubs(subtitleTextChunk)"
      ],
      "metadata": {
        "id": "oDyj_8KF1BNf",
        "outputId": "7fd5f2e3-e324-4022-8139-9295b94020b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0:0:0,140 --> 0:0:15,220\n",
            "Congratulations to you Mr Raghavan for that Thank you so much for joining us Over to you Hi everybody How are you I am not hearing this at all It's like a post lunch\n",
            "\n",
            "2\n",
            "0:0:15,220 --> 0:0:30,879\n",
            "energy downer or something Let's hear it Are you guys awake All right You better be because we have a superstar guest here You heard the 41 million and I didn't hear honestly anything she\n",
            "\n",
            "3\n",
            "0:0:30,879 --> 0:0:45,979\n",
            "said after that So we're going to ask for about 40 million from him by the end of this conversation But let's get started I want to introduce Vivek and Pratyush his co founder who's not here We\n",
            "\n",
            "4\n",
            "0:0:45,979 --> 0:1:60,819\n",
            "wanted to start with playing a video of what OpenHathi does I encourage all of you to go to the website Serum ai and check it out But let me start by introducing Vivek Vivek is a dear friend and\n",
            "\n",
            "5\n",
            "0:1:60,819 --> 0:1:75,839\n",
            "he's very very modest one of the most modest guys that I know But his personal journey Vivek you've been you got a PhD from Carnegie Mellon you started and sold a company to Magma And Vivek and I moved back to India from we were both in the valley on the\n",
            "\n",
            "6\n",
            "0:1:75,839 --> 0:1:90,180\n",
            "same day actually And you've been in India for the last 16 years And what most people don't know is your journey at Aadhar He spent 13 years selflessly at Aadhar Nobody would have heard of him\n",
            "\n",
            "7\n",
            "0:1:91,19 --> 0:1:105,620\n",
            "But he was a pioneering technology visionary behind Aadhar which we all take for granted today So please give it out So honestly when people when I think of selfless service truly selfless service\n",
            "\n",
            "8\n",
            "0:1:105,779 --> 0:2:120,680\n",
            "I always think of Vivek And since then he also was at AI for Bharat which we're going to touch on where he met Pratyush his other co founder Pratyush had a PhD from ETH at Zurich He was at IBM Research He was\n",
            "\n",
            "9\n",
            "0:2:120,680 --> 0:2:135,779\n",
            "at Microsoft Research playing a key role and a faculty at IIT Madras and at AI for Bharat So that's a little brief introduction about them These guys are modest modest engineers so they don't toot their own horn So forgive me for tooting their horn\n",
            "\n",
            "10\n",
            "0:2:135,779 --> 0:2:150,940\n",
            "in this case But let's jump right in about the money funding Forty one million bucks man that's a lot of money Right Every entrepreneur here is saying what the hell did these guys do What did the investors see to write\n",
            "\n",
            "11\n",
            "0:2:150,940 --> 0:2:165,979\n",
            "such a big check No I think I think it's I think it's a trend of the new trend of what's going on in India I think that for the very first time I think the investors have looked at you know let's try and build something deep tech out of the country\n",
            "\n",
            "12\n",
            "0:2:165,979 --> 0:3:180,820\n",
            "and let's try to figure out how to build something as a foundational technology out of the country And that's really what's what's really exciting You know and I think that about you know as as as well I was mentioning for the past\n",
            "\n",
            "13\n",
            "0:3:180,820 --> 0:3:195,800\n",
            "15 years I've been kind of working in kind of you know both digital public infrastructure and kind of a nonprofit kind of things And but when this whole thing of generative AI came about I said you know we said\n",
            "\n",
            "14\n",
            "0:3:195,860 --> 0:3:210,880\n",
            "OK how can I actually make a difference in this space And I said maybe this is the opportunity to actually come out and really build something You know and the only way that we realize that you can do it is actually in in in\n",
            "\n",
            "15\n",
            "0:3:210,880 --> 0:3:225,119\n",
            "the private sector And I think that's that And then we when we went out there and we said we want to build something which is a continuation Right I mean and fundamentally the question is the reason of what we want to do at Servum AI is we want to basically make generative\n",
            "\n",
            "16\n",
            "0:3:225,119 --> 0:4:240,600\n",
            "AI Available and accessible to the people in the country And that's that's the intent And when we said that we want to do this there was a resonance in the investment community And I think it's it's a responsibility to really to show that\n",
            "\n",
            "17\n",
            "0:4:240,600 --> 0:4:255,779\n",
            "that something like this can be built out of India So we see that as as as confidence and a responsibility And I also hope it's a trend that that you know that there are many more people like like us who are backed Because if you look at it maybe it's a large number in\n",
            "\n",
            "18\n",
            "0:4:255,779 --> 0:4:270,799\n",
            "a you know in the Indian context But in the global context I think that it's just there should be many many more entrepreneurs who are back to do things in India I'm going to come back to the many more entrepreneurs I'm obviously going to ask you about Babish's Krutrim\n",
            "\n",
            "19\n",
            "0:4:271,160 --> 0:4:285,940\n",
            "So we're going to come back to that question But again 41 million dollars I mean all of what you said you know two million dollars You know that's a good amount of money for a startup which you know which has not yet built anything What are you going to do with\n",
            "\n",
            "20\n",
            "0:4:285,940 --> 0:5:300,899\n",
            "all this money I can have a perfect solution for the problem I think in the last week I've got lots of calls from lots of people telling me how I can No but I know you first OK I'll be landed in the country the same day\n",
            "\n",
            "21\n",
            "0:5:301,140 --> 0:5:315,540\n",
            "I'm in the front of the queue No but but but honestly I think the key thing in this is is to putting together an amazing team And we actually have an amazing team but we believe that it is talent that will drive this kind of thing And so it\n",
            "\n",
            "22\n",
            "0:5:315,540 --> 0:5:330,739\n",
            "is it is to get get key talent And of course the other thing is compute This is extremely expensive compute wise to actually do these kinds of things And I think that those are the two primary things that that you know we'd use this for\n",
            "\n",
            "23\n",
            "0:5:331,260 --> 0:5:345,899\n",
            "OK I'm computing in my own head as an entrepreneur talent OK you have like 20 15 people How much are you paying these guys But OK well you won't touch on that But let's talk about what you guys actually built What is what is open hearty How would you explain\n",
            "\n",
            "24\n",
            "0:5:345,899 --> 0:6:360,880\n",
            "open hearty to many people here who might not have known about it So I think open hearty is So first of all right We come from I personally come from the open source ecosystem and we and also from the DPI ecosystem So we\n",
            "\n",
            "25\n",
            "0:6:360,880 --> 0:6:375,920\n",
            "believe that for this to work we need the ecosystem to be successful And as a result of that one of the first things we did was hey there are these open source large language models that exist Right I mean everybody knows about the Lama family from Meta\n",
            "\n",
            "26\n",
            "0:6:376,420 --> 0:6:390,779\n",
            "They also there are others like Mistral There are a bunch of open source you know large language models And then we said is there any way that they can existing open source model and teach it language skills Right\n",
            "\n",
            "27\n",
            "0:6:390,859 --> 0:6:405,899\n",
            "I mean and that is really the you know what we decide what we said that can we do something like that And is this a relatively frugal way of of actually you know making models you know work\n",
            "\n",
            "28\n",
            "0:6:405,899 --> 0:7:420,640\n",
            "in in diverse languages Because the truth is still today I mean if you look at the amount of data and knowledge it is still English dominates these things And I think that how do you actually take and make it understand Indian language understand Indian\n",
            "\n",
            "29\n",
            "0:7:420,640 --> 0:7:435,959\n",
            "context and all of those things in actually a in an efficient way And therefore this was an attempt to do that And it's a open hearty is you know is is currently based on the Lama seven billion model But we'll be releasing many more models in\n",
            "\n",
            "30\n",
            "0:7:435,959 --> 0:7:450,700\n",
            "different languages different sizes and things like that as part of this as part of this series And of course you know we will be building further models on those and doing other things to to to actually And we'll also have endpoints that people can use So that is not\n",
            "\n",
            "31\n",
            "0:7:450,700 --> 0:7:465,140\n",
            "it's definitely you know something that people can can can use to things And the that's that's the essence of of what what this open hearty is So what does it mean to people in the audience here who are either doing their own startups or a business or\n",
            "\n",
            "32\n",
            "0:7:465,140 --> 0:8:480,600\n",
            "or developers How should they look at open Sorry No no no I think I think the way you look at it is that we are one of the important things that we are doing is we're not just building models\n",
            "\n",
            "33\n",
            "0:8:481,399 --> 0:8:495,859\n",
            "We are also going to be building a platform a platform for developers where you can actually use a combination of various different kinds of models some which are from us some which are open source some which may not be open source And actually to actually\n",
            "\n",
            "34\n",
            "0:8:495,859 --> 0:8:510,660\n",
            "pull together and figure out how to deploy you know generative applications at scale and understand and evaluate their performance in an efficient manner And that's something that we are planning to do at this And this platform is\n",
            "\n",
            "35\n",
            "0:8:511,220 --> 0:8:525,739\n",
            "you know in the next couple of months will be coming out there It will be available to developers But of course those who want to start with the open source things and hack for that of course please go ahead and do that as well That's phenomenal But how does it\n",
            "\n",
            "36\n",
            "0:8:525,739 --> 0:9:540,840\n",
            "compare to open itself or Google See at least the things that we are doing now Right I mean one of the things that when we thought about building server we said we want to build a full stack generated\n",
            "\n",
            "37\n",
            "0:9:540,840 --> 0:9:555,700\n",
            "by a company and different people And our understanding of a stack is that we need to know how to train models from scratch We need to know how to kind of figure out how to deploy models to solve real world use cases And we need to play in the\n",
            "\n",
            "38\n",
            "0:9:555,700 --> 0:9:570,700\n",
            "ecosystem to make sure that we can actually deploy population scale applications Right So we were thinking about all of these things But still the models we were talking about are you know fairly small models They are fairly small models Right\n",
            "\n",
            "39\n",
            "0:9:570,799 --> 0:9:585,960\n",
            "The seven to maybe up to 70 billion kind of range we're talking about While these models like open AI and Google are obviously much bigger models Right But we want to understand the techniques and be able to build that muscle to do\n",
            "\n",
            "40\n",
            "0:9:585,960 --> 0:10:600,719\n",
            "all of these things to make it available to people Now those models are I mean as I said you know I think that there is space for all of those things And I think as even Sridhar was talking about earlier in the day\n",
            "\n",
            "41\n",
            "0:10:601,39 --> 0:10:615,780\n",
            "we believe that these smaller models can do very many many kind of domain specific tasks extremely well probably even better than the larger models And that is really one of the key areas And so therefore the\n",
            "\n",
            "42\n",
            "0:10:615,780 --> 0:10:630,820\n",
            "value of these kinds of things Right We are not aiming in these set of models to build any AGI Right That's not our goal here Our goal is to make things that work extremely well for domain specific use cases or increase accessibility\n",
            "\n",
            "43\n",
            "0:10:630,820 --> 0:10:645,760\n",
            "through language and all of those kinds of things And obviously all of this unique to India But what is unique about India I mean like what is is anything special in our ecosystem that that makes small models focused with Indian languages better\n",
            "\n",
            "44\n",
            "0:10:645,760 --> 0:11:660,799\n",
            "for more suited for our problems So I think that I mean there are quite a few things that are unique about India Right The first thing is I think that we are a voice first nation So therefore I think voice has to be the core to\n",
            "\n",
            "45\n",
            "0:11:660,799 --> 0:11:675,780\n",
            "doing things The other thing of course India is extremely it's a cost conscious country from a cost perspective Now I would say that there are lots of interesting use cases where you can use open AI and the cost\n",
            "\n",
            "46\n",
            "0:11:675,780 --> 0:11:690,700\n",
            "structure works that when depending on your application But when you want to scale things to a massive level and make it work then you have to figure out how small models work So that's something that is also specific to India The third thing which is specific to India is really\n",
            "\n",
            "47\n",
            "0:11:690,700 --> 0:11:705,700\n",
            "the success that India has had in building all this digital public infrastructure When you add the AI layer on top of it then you can actually get dramatic you know dramatic I think multiplicative combinatorial\n",
            "\n",
            "48\n",
            "0:11:705,700 --> 0:11:719,940\n",
            "effects based on doing things like that That's a phenomenal point Like you know it's like DPI to the power of AI almost in some ways And as a part of Aadhaar building Aadhaar no better person than you So in summary what I'm hearing is small models\n",
            "\n",
            "49\n",
            "0:11:719,940 --> 0:12:735,820\n",
            "specialized with trained with Indic specific language data suited for Indian problems at a compelling cost point will be suited for us We're not solving some world autonomous vehicles or some complex problem We're solving some basic problems specifically focused\n",
            "\n",
            "50\n",
            "0:12:735,820 --> 0:12:740,539\n",
            "on voice with multiple languages That is what you see as the future Am I paraphrasing\n",
            "\n",
            "51\n",
            "0:12:740,539 --> 0:12:765,799\n",
            "this correctly Fair enough\n",
            "\n",
            "52\n",
            "0:12:766,340 --> 0:12:775,340\n",
            "So coming back to the elephant in the room no no pun intended with open Hathi What about Babesh Agrawal and Kruthvim What is your take on that\n",
            "\n",
            "53\n",
            "0:13:811,840 --> 0:13:812,200\n",
            "One\n",
            "\n",
            "54\n",
            "0:13:812,200 --> 0:13:812,239\n",
            "more\n",
            "\n",
            "55\n",
            "0:13:812,239 --> 0:13:825,919\n",
            "question and then I want to talk about some of the predictions that you've boldly made So Vivek I usually ask people about what do you think the future is going to look like What do you think the future will be And everybody usually hedges I asked Vivek what do you think is going to happen by December 2024 What do you think sitting in this\n",
            "\n",
            "56\n",
            "0:13:825,919 --> 0:14:840,960\n",
            "room one year later we can expect And he made three bold predictions So I want to talk about that Before that I have one last question What are the top three applications that you think are relevant for India You heard Sridhar talk about medical Quick\n",
            "\n",
            "57\n",
            "0:14:840,960 --> 0:14:855,799\n",
            "summary what do you think the top three apps are for India for AI So I mean I think that as you said things like education and medical are clearly areas where I think that things can be leveraged The whole\n",
            "\n",
            "58\n",
            "0:14:855,799 --> 0:14:870,520\n",
            "idea of all these kind of the DPI aspect of it is another major application where things can happen And here I'm talking about country specific And I think the whole idea which Sridhar also talked about was the concept of software right And I think\n",
            "\n",
            "59\n",
            "0:14:870,520 --> 0:14:885,880\n",
            "that and clearly we have a very large software industry and how to reimagine those things in this context is also something that's going to be Fair enough Are you guys ready for Vivek Raghavan's bold predictions Yes No I'm\n",
            "\n",
            "60\n",
            "0:14:885,880 --> 0:15:900,900\n",
            "not hearing any yeses This is like a big deal He's like one of the smartest guys that I know He wants to make three predictions You don't want to hear it All right So I asked him what do you think you know a year later what do you think we can expect And he came up with three things\n",
            "\n",
            "61\n",
            "0:15:901,0 --> 0:15:915,760\n",
            "And usually people give very blah answers when you ask questions like this because they don't want to be caught wrong Not Vivek Vivek is bold So he basically said three things and I'm going to list out the three things and then ask him about it So number one he says I would prefer\n",
            "\n",
            "62\n",
            "0:15:915,760 --> 0:15:930,859\n",
            "to talk to an automated customer service than a real person because they'll give me a better answer So that is Vivek Raghavan's prediction number one So number two is that when everybody is talking about a GPU shortage Vivek predicts that\n",
            "\n",
            "63\n",
            "0:15:930,859 --> 0:15:945,919\n",
            "there'll be a GPU glut in India He thinks there'll be too much GPU Okay So if you want to short Nvidia stock this is a good time And number three which was extremely unexpected he said some companies will suddenly die Okay So Vivek\n",
            "\n",
            "64\n",
            "0:15:945,979 --> 0:16:960,99\n",
            "these are not what I expected So do you want to quickly talk about each of them why you just came up with these and then we'll throw the open for audience questions So I don't think I quite said it the way that\n",
            "\n",
            "65\n",
            "0:16:960,280 --> 0:16:975,619\n",
            "but it's interesting But I think that the first thing that we said is I think that and I don't think that this is I think that there will come a time when you know in\n",
            "\n",
            "66\n",
            "0:16:975,619 --> 0:16:990,940\n",
            "areas of customer service et cetera when you want to do something very specific Today you know when you call some kind of a bot you actually end up you mostly try to disconnect the call or you know you're extremely upset that you're talking to a bot\n",
            "\n",
            "67\n",
            "0:16:990,960 --> 0:16:1005,799\n",
            "But I think that there will come a time and I'm predicting it is sooner than later that you will actually get better responses from the bot than what the human representative at least the average human representative that you could talk to could give And\n",
            "\n",
            "68\n",
            "0:16:1005,799 --> 0:17:1020,99\n",
            "I think that that's just a so I just said that there will come a time where you know it's not a human you're talking to but it's probably more likely to solve your intent than the human person That's just something that\n",
            "\n",
            "69\n",
            "0:17:1020,99 --> 0:17:1035,839\n",
            "I think that could happen Okay Definitely controversial but we'll let it go What about the GPU glut No no yeah so I don't think that so I think that the fact that there is a tremendous shortage right now I think that\n",
            "\n",
            "70\n",
            "0:17:1035,839 --> 0:17:1039,540\n",
            "shortage will ease because that is how the cycles of things go\n",
            "\n",
            "71\n",
            "0:17:1039,619 --> 0:17:1065,380\n",
            "right There are many many more interesting problems that people will be able to solve I still remember you know we were\n",
            "\n",
            "72\n",
            "0:17:1065,380 --> 0:18:1080,880\n",
            "at a Gen AI event in Bangalore and we were talking to people and we said you know how many people have access to you know four A 100s this was the question that I'd asked and nobody in the room and these are all extremely enthusiastic Gen AI people and nobody\n",
            "\n",
            "73\n",
            "0:18:1080,880 --> 0:18:1095,939\n",
            "had access And I think that thing is going to change You will be able to get these kinds of things and people who want to hack and do things will have access to these things without you know having to write a you know a major check So Vivek is also a\n",
            "\n",
            "74\n",
            "0:18:1095,939 --> 0:18:1107,760\n",
            "semiconductor guy before he went into Aadhaar so I would take his predictions very seriously so I don't know what I'm going to sell my NVIDIA stock But the third\n",
            "\n",
            "75\n",
            "0:18:1107,760 --> 0:18:1125,859\n",
            "one is pretty strange You know companies are born companies die but you said some companies will suddenly die What does that mean No I think see I think the interesting thing is and I think that it comes back to the fundamental\n",
            "\n",
            "76\n",
            "0:18:1125,859 --> 0:19:1140,439\n",
            "nature of AI AI is a tool right and you have to use that and you have to use that within your business process right And how AI is being used and so and what's going to happen is that I mean I think this is true\n",
            "\n",
            "77\n",
            "0:19:1140,439 --> 0:19:1155,900\n",
            "with you know when someone said in terms of you know people they said that the people who leverage AI will be more effective than those who don't leverage AI And that was true for organizations also Organizations that\n",
            "\n",
            "78\n",
            "0:19:1155,900 --> 0:19:1170,800\n",
            "leverage AI in fundamentally in their core business processes will be more effective than those who don't right And I think that's the thing and you won't know the difference until one day it becomes too obvious and it will be too late And I think\n",
            "\n",
            "79\n",
            "0:19:1170,800 --> 0:19:1185,780\n",
            "that's the reason why everybody needs to think about what it means for your business because you will everything will be fine Everything will be fine and one day somebody in your either your competitor in your space or\n",
            "\n",
            "80\n",
            "0:19:1185,780 --> 0:20:1200,459\n",
            "somebody brand new coming into your space will be reimagining your business process completely And at that stage you will find that it's you know it's a very big very tall you know mountain to climb And that's why I think it's important\n",
            "\n",
            "81\n",
            "0:20:1200,459 --> 0:20:1215,959\n",
            "for both people and entities to think about how they will you know they will upgrade themselves or they will modify their business processes to you know should really think about it because life\n",
            "\n",
            "82\n",
            "0:20:1215,959 --> 0:20:1230,780\n",
            "will be the same And then suddenly suddenly something will you know then there'll be a step change Vivek I have a few more questions but I'm sure the audience has a lot of questions for you So how are we doing on time Okay so does\n",
            "\n",
            "83\n",
            "0:20:1230,780 --> 0:20:1245,939\n",
            "Okay a lot of questions So love to Is there a mic that we can pass around Thank you My name is Karthik I work for IT service industry\n",
            "\n",
            "84\n",
            "0:20:1246,160 --> 0:21:1260,380\n",
            "So you're saying that you're working on LLM sorry it's a fine tuned LLM on top of Lama My basic question fundamental question is we don't have a fundamental foundational model for India most of the models\n",
            "\n",
            "85\n",
            "0:21:1260,380 --> 0:21:1275,699\n",
            "are basically using English or those kind of things For example even Andrew was talking about the tokenizers and things like that So are you working on anything like that Or you do you want to use mostly the existing\n",
            "\n",
            "86\n",
            "0:21:1275,699 --> 0:21:1290,300\n",
            "models and run on top of it Are you going to No I think the interesting thing is that if you look at and then we have actually a blog on this on our website I think one of the things that we've actually built\n",
            "\n",
            "87\n",
            "0:21:1290,300 --> 0:21:1305,839\n",
            "a customized tokenizer which actually fundamentally changes the cost of some of these generations in Indian languages And I think that we're not just fine tuning we're actually we are leveraging the existing pre training but we are doing\n",
            "\n",
            "88\n",
            "0:21:1305,839 --> 0:22:1320,859\n",
            "what's known as continual free training which actually allows us to run on top of it But having said that you know I think that when we have to figure out where is the data to train an extremely large model from scratch and some of those things are things which will happen over time But I\n",
            "\n",
            "89\n",
            "0:22:1320,859 --> 0:22:1335,800\n",
            "think that I think that yes I think that we will be doing various kinds of things But the interesting thing is that if I want to change the accessibility problem with an existing open source model how do I do that And that's the problem that we\n",
            "\n",
            "90\n",
            "0:22:1335,800 --> 0:22:1350,959\n",
            "have that we think we have solved and is going to be the heart of this open RTC Extremely well explained in the blog even I could understand it Hi I'm Prashant I work for a fintech company My question is like unlike China we\n",
            "\n",
            "91\n",
            "0:22:1350,959 --> 0:22:1364,180\n",
            "never had a consumer facing application coming out from India and in web one web two crypto and all Why do you think it will be different this time in like AI\n",
            "\n",
            "92\n",
            "0:22:1365,819 --> 0:23:1380,260\n",
            "Because will the DPI and other things will serve the same purpose what the Great Firewall did in China Or do you think like in because AI is a strategic sector no outside country can work\n",
            "\n",
            "93\n",
            "0:23:1380,260 --> 0:23:1395,640\n",
            "in NASA projects maybe all government contract will go to them What exactly is the mode here for an Indian company So I think the question is\n",
            "\n",
            "94\n",
            "0:23:1396,479 --> 0:23:1410,680\n",
            "I don't know the answer to these questions right I mean I think that it's difficult to predict But I do believe and as I'm repeating that the combinatorial effect of being using Gen AI at a large scale in addition\n",
            "\n",
            "95\n",
            "0:23:1411,420 --> 0:23:1425,219\n",
            "along with the DPI work that we've done in India will have people And I think that in the end it is the intent is that people need to be able to use it and they will vote by things that are useful for them And if that doesn't happen\n",
            "\n",
            "96\n",
            "0:23:1425,619 --> 0:24:1440,439\n",
            "you're right that I think that we have to figure out what is the mechanism of delivery of apps right I mean how where do Indians consume content That's the question I'm so sorry but we are out of time Vivek will be outside\n",
            "\n",
            "97\n",
            "0:24:1441,280 --> 0:24:1444,180\n",
            "So he would be able to answer the question Do we have time for one last question\n",
            "\n",
            "98\n",
            "0:24:1467,199 --> 0:24:1470,979\n",
            "There is one thing that they have been regularly that the concentrations that they are\n",
            "\n",
            "99\n",
            "0:24:1470,979 --> 0:24:1485,819\n",
            "working on but artificial intelligence and getting into this getting them into their academics and making them a part of it is very important including the trainers who train them making them future ready into what you are doing is amazing And the speed that which is\n",
            "\n",
            "100\n",
            "0:24:1485,819 --> 0:25:1500,979\n",
            "growing it is calling for a lot of training that needs to be done Can you from your angle throw some light on how we could make them future ready How these people who are management graduates and from schools who are coming out how do\n",
            "\n",
            "101\n",
            "0:25:1500,979 --> 0:25:1503,599\n",
            "we get into this part of technology that you spoke about\n",
            "\n",
            "102\n",
            "0:25:1520,119 --> 0:25:1530,959\n",
            "I think it has to be at many different levels right There are from a core set of having people who are extremely good at some and there you don't need as many but then there are\n",
            "\n",
            "103\n",
            "0:25:1530,959 --> 0:25:1545,900\n",
            "basically vast numbers of people who can actually leverage these tools By the way the most important thing about and maybe that's part of what makes an LLM interesting is that how you use it your mileage varies by that And to understand how\n",
            "\n",
            "104\n",
            "0:25:1545,900 --> 0:26:1560,640\n",
            "to actually leverage this in an interesting way is something that we have to widely teach many many people and because asking the you know things in the right way and having the right kind of applications will make a huge difference to\n",
            "\n",
            "105\n",
            "0:26:1560,640 --> 0:26:1571,400\n",
            "how people can leverage these tools Awesome Thank you Thank you very much Vivek Very good luck to Sarvam and good luck to India I think it's going to be a lot riding on your shoulders Thanks Bala\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}